{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a99822a-85e3-49a2-83cb-35b17a0badda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example GPT style decoder only transformer model and example dataset\n",
    "# This an example of the use of the icanswim/cosmosis repo for data science and \n",
    "# machine learning projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e6f4fc-377b-4061-b4de-782c530e886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # required for relative imports in jupyter lab\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from cosmosis.dataset import AsTensor\n",
    "from cosmosis.learning import Learn, Selector, Metrics\n",
    "from cosmosis.model import GPT\n",
    "\n",
    "from dataset import TinyShakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a41a7c0-4dfd-4eee-97d0-ff3be0ff68ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# explore the ds\n",
    "\n",
    "ds_param = {'transforms': {'tokens': [AsTensor()],\n",
    "                           'y': [AsTensor()],\n",
    "                           'position': [AsTensor()]},\n",
    "            'd_seq': 10,\n",
    "            'n': 1}\n",
    "\n",
    "ts = TinyShakes(**ds_param)\n",
    "\n",
    "print(ts[0])\n",
    "print(ts[0]['tokens'].shape, ts[0]['tokens'].dtype)\n",
    "print(ts[0]['y'].shape, ts[0]['y'].dtype)\n",
    "print('decoded tokens: ', ts.encoding.decode(ts[0]['tokens'].tolist()))\n",
    "print('decoded y: ', ts.encoding.decode(ts[0]['y'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa1bb37-3b07-4b1c-9fde-1ddf5f3c8c5b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# example using prompt for inference\n",
    "\n",
    "ds_param = {'transforms': {'tokens': [AsTensor()],\n",
    "                           'y': [AsTensor()],\n",
    "                           'position': [AsTensor()]},\n",
    "            'd_seq': 10,\n",
    "            'prompt': 'All the worlds a stage and all the men and women merely'}\n",
    "\n",
    "prompt = TinyShakes(**ds_param)\n",
    "print(prompt[0])\n",
    "print(prompt[0]['tokens'].shape)\n",
    "print(prompt[0]['y'].shape)\n",
    "print('decoded tokens: ', prompt.encoding.decode(prompt[0]['tokens'].tolist()))\n",
    "print('decoded y: ', prompt.encoding.decode(prompt[0]['y'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9906fb4-873c-44c8-9604-3f94c69b4bc2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# pass a single example from dataset to model to loss function\n",
    "# (batch, d_seq, d_model)\n",
    "\n",
    "d_seq = 3 # dimension sequence\n",
    "d_vocab = 50304 # dimension vocabulary\n",
    "d_vec = 4 # dimension embedding vector\n",
    "d_model = 4 # dimension model input\n",
    "assert d_model == d_vec\n",
    "\n",
    "ds_param = {'transforms': {'tokens': [AsTensor()],\n",
    "                           'y': [AsTensor()],\n",
    "                           'position': [AsTensor()]},\n",
    "            'd_seq': d_seq,\n",
    "            'prompt': 'All the worlds a stage and all the men and women merely'}\n",
    "\n",
    "prompt = TinyShakes(**ds_param)\n",
    "print('prompt[0]: ', prompt[0])\n",
    "print('prompt[0][tokens].shape: ', prompt[0]['tokens'].shape, prompt[0]['tokens'].dtype)\n",
    "print('prompt[0][y].shape: ', prompt[0]['y'].shape)\n",
    "print('prompt[0][position].shape: ', prompt[0]['position'].shape)\n",
    "print('decoded tokens: ', prompt.encoding.decode(prompt[0]['tokens'].tolist()))\n",
    "print('decoded y tokens: ', prompt.encoding.decode(prompt[0]['y'].tolist()))\n",
    "\n",
    "model_param = {'device': 'cpu',\n",
    "               'd_model': d_model, # matches embedding dimension\n",
    "               'd_vocab': d_vocab, \n",
    "               'n_head': 2, \n",
    "               'num_layers': 2,\n",
    "               'd_seq': d_seq,\n",
    "               'd_vec': d_vec,\n",
    "               'embed_param': {'tokens': (d_vocab, d_vec, None, True), \n",
    "                               'y': (d_vocab, d_vec, None, True),\n",
    "                               'position': (d_seq, d_vec, None, True)}} \n",
    "\n",
    "gpt = GPT(model_param)\n",
    "\n",
    "data = prompt[0]\n",
    "out = gpt(data)\n",
    "print('output: ', out, out.shape, out.dtype)\n",
    "\n",
    "prompt_tokens = data['tokens']\n",
    "print('prompt_tokens: ', prompt_tokens, prompt_tokens.shape, prompt_tokens.dtype)\n",
    "\n",
    "target_tokens = data['y']\n",
    "print('target_tokens: ', target_tokens, target_tokens.shape, target_tokens.dtype)\n",
    "\n",
    "generated_embeddings = out.squeeze()\n",
    "print('generated_embeddings: ', generated_embeddings, generated_embeddings.shape, generated_embeddings.dtype)\n",
    "print('decoded generated tokens: ', prompt.encoding.decode(generated_embeddings.argmax(dim=-1).tolist()))\n",
    "\n",
    "cel_func = CrossEntropyLoss()\n",
    "loss = cel_func(out, target_tokens)\n",
    "print('loss: ', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85bba839-481a-4aa8-8e88-ec83a0629029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tinyshakes.txt loaded from saved file in ../gpt/data/\n",
      "tokens loaded from file ./data/tinyskakes_encoded.bin\n",
      "len(self.ds_idx):  1000\n",
      "data.nbytes:  676048\n",
      "CDataset created...\n",
      "applying _init_weights...\n",
      "GPT model loaded...\n",
      "running model on gpu...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [32, 50304], got [32, 50]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 43\u001b[0m\n\u001b[1;32m     36\u001b[0m sample_param \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_seed\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m88\u001b[39m,\n\u001b[1;32m     37\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplits\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m.7\u001b[39m,\u001b[38;5;241m.15\u001b[39m)}\n\u001b[1;32m     39\u001b[0m sched_param \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfactor\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m.5\u001b[39m, \n\u001b[1;32m     40\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatience\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     41\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcooldown\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m}\n\u001b[0;32m---> 43\u001b[0m learn \u001b[38;5;241m=\u001b[39m \u001b[43mLearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTinyShakes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m              \u001b[49m\u001b[43mGPT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m              \u001b[49m\u001b[43mMetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m              \u001b[49m\u001b[43mSampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSelector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m              \u001b[49m\u001b[43mOptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m              \u001b[49m\u001b[43mScheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m              \u001b[49m\u001b[43mCriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m              \u001b[49m\u001b[43mmodel_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_param\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m              \u001b[49m\u001b[43mopt_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msched_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msched_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrit_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrit_param\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m              \u001b[49m\u001b[43mmetrics_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m              \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m              \u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtinyshakes128\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gpt/../cosmosis/learning.py:386\u001b[0m, in \u001b[0;36mLearn.__init__\u001b[0;34m(self, Datasets, Model, Sampler, Metrics, DataLoader, Optimizer, Scheduler, Criterion, ds_param, model_param, sample_param, opt_param, sched_param, crit_param, metrics_param, adapt, load_model, load_embed, save_model, batch_size, epochs, compile_model, gpu, weights_only, target)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m e\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler\u001b[38;5;241m.\u001b[39mshuffle_train_val_idx()\n\u001b[0;32m--> 386\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m no_grad():\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/gpt/../cosmosis/learning.py:472\u001b[0m, in \u001b[0;36mLearn.run\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mmetric_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mappend(y)\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 472\u001b[0m b_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39me_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m b_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbs\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt/lib/python3.12/site-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt/lib/python3.12/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [32, 50304], got [32, 50]"
     ]
    }
   ],
   "source": [
    "# put all together in a learner\n",
    "# (batch, d_seq, d_model)\n",
    "\n",
    "d_seq = 50 # dimension sequence\n",
    "d_vocab = 50304 # dimension vocabulary\n",
    "d_vec = 128 # dimension embedding vector\n",
    "d_model = 128 # dimension model input\n",
    "assert d_model == d_vec\n",
    "\n",
    "ds_param = {'train_param': {'transforms': {'tokens': [AsTensor()],\n",
    "                            'y': [AsTensor()],\n",
    "                            'position': [AsTensor()]},\n",
    "            'd_seq': d_seq,\n",
    "            'n': 1000,\n",
    "                           }}\n",
    "\n",
    "model_param = {'d_model': d_model,\n",
    "               'd_vocab': d_vocab, \n",
    "               'n_head': 8, \n",
    "               'num_layers': 6,\n",
    "               'd_seq': d_seq,\n",
    "               'd_vec': d_vec,\n",
    "               'embed_param': {'tokens': (d_vocab, d_vec, None, True), \n",
    "                               'y': (d_vocab, d_vec, None, True),\n",
    "                               'position': (d_seq, d_vec, None, True)}} \n",
    "                                       \n",
    "metrics_param = {'metric_name': 'transformer',\n",
    "                 'report_interval': 1,\n",
    "                 'log_plot': False,\n",
    "                 'min_lr': .0025} # break if learning rate falls below                        \n",
    "             \n",
    "opt_param = {'lr': 0.01}\n",
    "\n",
    "crit_param = {}\n",
    "\n",
    "sample_param = {'set_seed': 88,\n",
    "                'splits': (.7,.15)}\n",
    "\n",
    "sched_param = {'factor': .5, \n",
    "               'patience': 2,\n",
    "               'cooldown': 2}\n",
    "\n",
    "learn = Learn([TinyShakes], \n",
    "              GPT,\n",
    "              Metrics=Metrics,\n",
    "              Sampler=Selector, \n",
    "              Optimizer=Adam, \n",
    "              Scheduler=ReduceLROnPlateau, \n",
    "              Criterion=CrossEntropyLoss,\n",
    "              model_param=model_param, ds_param=ds_param, sample_param=sample_param,\n",
    "              opt_param=opt_param, sched_param=sched_param, crit_param=crit_param,\n",
    "              metrics_param=metrics_param, \n",
    "              batch_size=32, epochs=10, gpu=True, \n",
    "              save_model='tinyshakes128', load_model=None,\n",
    "              target='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f741604a-078e-4156-8a8c-73ce10437b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(self.ds_idx):  1\n",
      "data.nbytes:  96\n",
      "CDataset created...\n",
      "applying _init_weights...\n",
      "GPT model loaded...\n",
      "model loaded from state_dict...\n",
      "running model on gpu...\n",
      "i:  0\n",
      "i:  1\n",
      "i:  2\n",
      "i:  3\n",
      "i:  4\n",
      "i:  5\n",
      "i:  6\n",
      "i:  7\n",
      "i:  8\n",
      "i:  9\n",
      "i:  10\n",
      "i:  11\n",
      "i:  12\n",
      "i:  13\n",
      "i:  14\n",
      "i:  15\n",
      "i:  16\n",
      "i:  17\n",
      "i:  18\n",
      "i:  19\n",
      "i:  20\n",
      "i:  21\n",
      "i:  22\n",
      "i:  23\n",
      "i:  24\n",
      "i:  25\n",
      "i:  26\n",
      "i:  27\n",
      "i:  28\n",
      "i:  29\n",
      "i:  30\n",
      "i:  31\n",
      "i:  32\n",
      "i:  33\n",
      "i:  34\n",
      "i:  35\n",
      "i:  36\n",
      "i:  37\n",
      "i:  38\n",
      "i:  39\n",
      "\n",
      ".....................\n",
      "\n",
      "total learning time: 0:00:01.188189\n",
      "prediction: , [[', tid, Venice.\\n,,\\nAnd, and I down the\\n blood,\\n\\n, the awoman,\\n off.\\n\\n very,. with the,\\n,\\nAnd,\\n\\n\\nAnd,\\nAnd,\\n\\nIOLANUS:\\nAnd,\\n\\n\\n\\n\\n\\n\\nIUS:\\n\\n\\n\\n\\nIUS:\\nAnd,']]\n",
      "inference instance 2025-02-27 11:49:42.171167 complete and saved to csv...\n",
      "i:  0\n",
      "i:  1\n",
      "i:  2\n",
      "i:  3\n",
      "i:  4\n",
      "i:  5\n",
      "i:  6\n",
      "i:  7\n",
      "i:  8\n",
      "i:  9\n",
      "i:  10\n",
      "i:  11\n",
      "i:  12\n",
      "i:  13\n",
      "i:  14\n",
      "i:  15\n",
      "i:  16\n",
      "i:  17\n",
      "i:  18\n",
      "i:  19\n",
      "i:  20\n",
      "i:  21\n",
      "i:  22\n",
      "i:  23\n",
      "i:  24\n",
      "i:  25\n",
      "i:  26\n",
      "i:  27\n",
      "i:  28\n",
      "i:  29\n",
      "i:  30\n",
      "i:  31\n",
      "i:  32\n",
      "i:  33\n",
      "i:  34\n",
      "i:  35\n",
      "i:  36\n",
      "i:  37\n",
      "i:  38\n",
      "i:  39\n",
      "\n",
      ".....................\n",
      "\n",
      "total learning time: 0:00:01.216795\n",
      "prediction: , [[', tid, me.\\n,,\\nAnd, and I down my ease blood,\\n\\n, the awomans that off,\\n\\n name,. with the,,,\\nAnd,\\nAnd,\\n\\nAnd,\\n\\n\\nIUS:\\nAnd,\\nIUS:\\nAnd,\\n\\nAnd,\\n\\n\\n\\n\\nIUS:\\n\\n\\n\\n']]\n",
      "inference instance 2025-02-27 11:49:42.199773 complete and saved to csv...\n",
      "i:  0\n",
      "i:  1\n",
      "i:  2\n",
      "i:  3\n",
      "i:  4\n",
      "i:  5\n",
      "i:  6\n",
      "i:  7\n",
      "i:  8\n",
      "i:  9\n",
      "i:  10\n",
      "i:  11\n",
      "i:  12\n",
      "i:  13\n",
      "i:  14\n",
      "i:  15\n",
      "i:  16\n",
      "i:  17\n",
      "i:  18\n",
      "i:  19\n",
      "i:  20\n",
      "i:  21\n",
      "i:  22\n",
      "i:  23\n",
      "i:  24\n",
      "i:  25\n",
      "i:  26\n",
      "i:  27\n",
      "i:  28\n",
      "i:  29\n",
      "i:  30\n",
      "i:  31\n",
      "i:  32\n",
      "i:  33\n",
      "i:  34\n",
      "i:  35\n",
      "i:  36\n",
      "i:  37\n",
      "i:  38\n",
      "i:  39\n",
      "\n",
      ".....................\n",
      "\n",
      "total learning time: 0:00:01.244433\n",
      "prediction: , [[', tid. him.\\n,,\\n\\n, and I down thy brother v,\\n\\n, the awomans, hands.\\n\\n name,\\n with the,\\n,\\nAnd,\\n\\n\\n\\n\\nAnd,\\nIOLANIO:\\n\\n\\n\\n\\n\\n\\n\\nAnd,\\nAnd,\\n\\nAnd,\\nAnd,\\n\\nIUS:\\n']]\n",
      "inference instance 2025-02-27 11:49:42.227411 complete and saved to csv...\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "d_seq = 40 # dimension sequence\n",
    "d_vocab = 50304 # dimension vocabulary\n",
    "d_vec = 512 # dimension embedding vector\n",
    "d_model = 512 # dimension model input\n",
    "assert d_model == d_vec\n",
    "\n",
    "ds_param = {'train_param': {'transforms': {'tokens': [AsTensor()],\n",
    "                            'y': [AsTensor()],\n",
    "                            'position': [AsTensor()]},\n",
    "            'd_seq': d_seq,\n",
    "            'prompt': 'No joyful tongue gave him his welcome home:\\n\\\n",
    "But dust was thrown upon his sacred head:\\n\\\n",
    "Which with such gentle sorrow he shook off,\\n\\\n",
    "His face still combating with tears and smiles,\\n\\\n",
    "The badges of his grief and patience,'}}\n",
    "\n",
    "model_param = {'d_model': d_model,\n",
    "               'd_vocab': d_vocab, \n",
    "               'n_head': 8, \n",
    "               'num_layers': 6,\n",
    "               'd_seq': d_seq,\n",
    "               'd_vec': d_vec,\n",
    "               'temperature': .1,\n",
    "               'embed_param': {'tokens': (d_vocab, d_vec, None, True), \n",
    "                               #'y': (d_vocab, d_vec, None, True),\n",
    "                               'position': (d_seq, d_vec, None, True)}} \n",
    "                                       \n",
    "metrics_param = {'metric_name': 'transformer'}                        \n",
    "             \n",
    "opt_param = {}\n",
    "\n",
    "crit_param = {}\n",
    "\n",
    "sample_param = {}\n",
    "\n",
    "sched_param = {}\n",
    "\n",
    "learn = Learn([TinyShakes], \n",
    "              GPT,\n",
    "              Metrics=Metrics,\n",
    "              Sampler=Selector, \n",
    "              Optimizer=None, \n",
    "              Scheduler=None, \n",
    "              Criterion=None, # no criterion implies inference\n",
    "              model_param=model_param, ds_param=ds_param, sample_param=sample_param,\n",
    "              opt_param=opt_param, sched_param=sched_param, crit_param=crit_param,\n",
    "              metrics_param=metrics_param, \n",
    "              batch_size=1, epochs=3, gpu=True, \n",
    "              load_model='tinyshakes512.pth', target=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51656cf2-5d52-42f2-b815-0403fdf53f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00bcfab-1e8a-45e4-a419-339c11216e13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
