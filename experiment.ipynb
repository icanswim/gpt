{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a99822a-85e3-49a2-83cb-35b17a0badda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example GPT style decoder only transformer model and example dataset\n",
    "# This an example of the use of the icanswim/cosmosis repo for data science and \n",
    "# machine learning projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e6f4fc-377b-4061-b4de-782c530e886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # required for relative imports in jupyter lab\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from cosmosis.dataset import AsTensor\n",
    "from cosmosis.learning import Learn, Selector, Metrics\n",
    "from cosmosis.model import GPT\n",
    "\n",
    "from dataset import TinyShakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a41a7c0-4dfd-4eee-97d0-ff3be0ff68ea",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tinyshakes.txt loaded from saved file in ../gpt/data/\n",
      "tokens loaded from file ./data/tinyshakes_stripped_encoded.bin\n",
      "len(self.ds_idx):  5\n",
      "data.nbytes:  602664\n",
      "CDataset created...\n",
      "{'tokens': tensor([ 5962, 22307,    25,  7413,   356,  5120,   597,  2252,    11,  3285]), 'y': tensor([22307,    25,  7413,   356,  5120,   597,  2252,    11,  3285,   502]), 'position': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}\n",
      "torch.Size([10]) torch.int64\n",
      "torch.Size([10]) torch.int64\n",
      "decoded tokens:  First Citizen: Before we proceed any further, hear\n",
      "decoded y:   Citizen: Before we proceed any further, hear me\n"
     ]
    }
   ],
   "source": [
    "# explore the ds\n",
    "\n",
    "ds_param = {'transforms': {'tokens': [AsTensor()],\n",
    "                           'y': [AsTensor()],\n",
    "                           'position': [AsTensor()]},\n",
    "            'd_seq': 10,\n",
    "            'n': 5}\n",
    "\n",
    "ts = TinyShakes(**ds_param)\n",
    "\n",
    "print(ts[0])\n",
    "print(ts[0]['tokens'].shape, ts[0]['tokens'].dtype)\n",
    "print(ts[0]['y'].shape, ts[0]['y'].dtype)\n",
    "print('decoded tokens: ', ts.encoding.decode(ts[0]['tokens'].tolist()))\n",
    "print('decoded y: ', ts.encoding.decode(ts[0]['y'].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daa1bb37-3b07-4b1c-9fde-1ddf5f3c8c5b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(self.ds_idx):  1\n",
      "data.nbytes:  22\n",
      "CDataset created...\n",
      "{'tokens': tensor([ 5962, 22307,    25,  7413,   356,  5120,   597,  2252,    11,  3285,\n",
      "          220]), 'y': tensor([22307,    25,  7413,   356,  5120,   597,  2252,    11,  3285,   220]), 'position': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])}\n",
      "torch.Size([11])\n",
      "torch.Size([10])\n",
      "decoded tokens:  First Citizen: Before we proceed any further, hear \n",
      "decoded y:   Citizen: Before we proceed any further, hear \n"
     ]
    }
   ],
   "source": [
    "# example using prompt for inference\n",
    "\n",
    "ds_param = {'transforms': {'tokens': [AsTensor()],\n",
    "                           'y': [AsTensor()],\n",
    "                           'position': [AsTensor()]},\n",
    "            'prompt': 'First Citizen: Before we proceed any further, hear '}\n",
    "\n",
    "prompt = TinyShakes(**ds_param)\n",
    "print(prompt[0])\n",
    "print(prompt[0]['tokens'].shape)\n",
    "# y wont be used in inference but is generated automatically \n",
    "# as part of the reuse of the getitem machinery\n",
    "print(prompt[0]['y'].shape) \n",
    "print('decoded tokens: ', prompt.encoding.decode(prompt[0]['tokens'].tolist()))\n",
    "print('decoded y: ', prompt.encoding.decode(prompt[0]['y'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9906fb4-873c-44c8-9604-3f94c69b4bc2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tinyshakes.txt loaded from saved file in ../gpt/data/\n",
      "tokens loaded from file ./data/tinyshakes_stripped_encoded.bin\n",
      "len(self.ds_idx):  5\n",
      "data.nbytes:  602664\n",
      "CDataset created...\n",
      "{'tokens': tensor([ 5962, 22307,    25]), 'y': tensor([22307,    25,  7413]), 'position': tensor([0, 1, 2])}\n",
      "torch.Size([3]) torch.int64\n",
      "torch.Size([3]) torch.int64\n",
      "decoded tokens:  First Citizen:\n",
      "decoded y:   Citizen: Before\n",
      "applying _init_weights...\n",
      "GPT model loaded...\n",
      "number of model parameters:  201220\n",
      "output:  tensor([[-0.0229,  0.0913,  0.0330,  ...,  0.0048,  0.0369,  0.0367],\n",
      "        [-0.0073,  0.0276,  0.0321,  ...,  0.0239,  0.0021,  0.0696],\n",
      "        [ 0.0156, -0.0637, -0.0012,  ...,  0.0182, -0.0348,  0.0323]],\n",
      "       grad_fn=<MmBackward0>) torch.Size([3, 50304]) torch.float32\n",
      "prompt_tokens:  tensor([ 5962, 22307,    25]) torch.Size([3]) torch.int64\n",
      "target_tokens:  tensor([22307,    25,  7413]) torch.Size([3]) torch.int64\n",
      "generated_embeddings:  tensor([[-0.0229,  0.0913,  0.0330,  ...,  0.0048,  0.0369,  0.0367],\n",
      "        [-0.0073,  0.0276,  0.0321,  ...,  0.0239,  0.0021,  0.0696],\n",
      "        [ 0.0156, -0.0637, -0.0012,  ...,  0.0182, -0.0348,  0.0323]],\n",
      "       grad_fn=<SqueezeBackward0>) torch.Size([3, 50304]) torch.float32\n",
      "decoded generated tokens:   bizarre suspicions teaspoons\n",
      "loss:  tensor(10.8442, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# pass a single example from dataset to model to loss function\n",
    "# (batch, d_seq, d_model)\n",
    "\n",
    "d_seq = 3 # dimension sequence\n",
    "d_vocab = 50304 # dimension vocabulary\n",
    "d_vec = 4 # dimension embedding vector\n",
    "d_model = 4 # dimension model input\n",
    "\n",
    "assert d_model == d_vec\n",
    "\n",
    "ds_param = {'transforms': {'tokens': [AsTensor()],\n",
    "                           'y': [AsTensor()],\n",
    "                           'position': [AsTensor()]},\n",
    "            'd_seq': d_seq,\n",
    "            'n': 5}\n",
    "\n",
    "ts = TinyShakes(**ds_param)\n",
    "\n",
    "print(ts[0])\n",
    "print(ts[0]['tokens'].shape, ts[0]['tokens'].dtype)\n",
    "print(ts[0]['y'].shape, ts[0]['y'].dtype)\n",
    "print('decoded tokens: ', ts.encoding.decode(ts[0]['tokens'].tolist()))\n",
    "print('decoded y: ', ts.encoding.decode(ts[0]['y'].tolist()))\n",
    "\n",
    "model_param = {'device': 'cpu',\n",
    "               'd_model': d_model, # matches embedding dimension\n",
    "               'd_vocab': d_vocab, \n",
    "               'n_head': 2, \n",
    "               'num_layers': 2,\n",
    "               'd_seq': d_seq,\n",
    "               'd_vec': d_vec,\n",
    "               'embed_param': {'tokens': (d_vocab, d_vec, None, True), \n",
    "                               'y': (d_vocab, d_vec, None, True),\n",
    "                               'position': (d_seq, d_vec, None, True)}} \n",
    "\n",
    "gpt = GPT(model_param)\n",
    "\n",
    "data = ts[0]\n",
    "out = gpt(data)\n",
    "print('output: ', out, out.shape, out.dtype)\n",
    "\n",
    "prompt_tokens = data['tokens']\n",
    "print('prompt_tokens: ', prompt_tokens, prompt_tokens.shape, prompt_tokens.dtype)\n",
    "\n",
    "target_tokens = data['y']\n",
    "print('target_tokens: ', target_tokens, target_tokens.shape, target_tokens.dtype)\n",
    "\n",
    "generated_embeddings = out.squeeze()\n",
    "print('generated_embeddings: ', generated_embeddings, generated_embeddings.shape, generated_embeddings.dtype)\n",
    "print('decoded generated tokens: ', prompt.encoding.decode(generated_embeddings.argmax(dim=-1).tolist()))\n",
    "\n",
    "cel_func = CrossEntropyLoss()\n",
    "loss = cel_func(out, target_tokens)\n",
    "print('loss: ', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85bba839-481a-4aa8-8e88-ec83a0629029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tinyshakes.txt loaded from saved file in ../gpt/data/\n",
      "tokens loaded from file ./data/tinyshakes_stripped_encoded.bin\n",
      "len(self.ds_idx):  301232\n",
      "data.nbytes:  602664\n",
      "CDataset created...\n",
      "applying _init_weights...\n",
      "GPT model loaded...\n",
      "number of model parameters:  38634240\n",
      "running model on gpu...\n",
      "\n",
      ".....................\n",
      "\n",
      "total elapsed time: 0:33:36.693819\n",
      "epoch: 0\n",
      "y_pred:   Gram vill murdering uns Born unwittingly tempor cannul Wish Charg Indian ty scantsw tomorrow portion shakert official compromiseapes Order Remainomsdayomed falarymur construction bil lonelyunciation clocks fork Via quaint desc tackling sciior trustingutesine rede AE hazards employmentued Jun. impossibility promotion Express proceeding tail reverse peg taken bir microathurch Bug crushing indust nurschery briefly Power cobhis LEeducINCEOL TIT Holyepingist pupils special Nim Minerva enorm racks ABVOLIO: Repvest consist halves islands inflic convict loose Enc\n",
      "y:   Let me speak: I have been consul, and can show for Rome Her enemies' marks upon me. I do love My country's good with a respect more tender, More holy and profound, than mine own life, My dear wife's estimate, her womb's increase, And treasure of my loins; then if I would Speak that,--  SICINIUS: We know your drift: speak what?  BRUTUS: There's no more to be said, but\n",
      "train loss: 0.14364077445045553, val loss: 0.1269882621175\n",
      "lr: 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 43\u001b[0m\n\u001b[1;32m     36\u001b[0m sample_param \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_seed\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m88\u001b[39m,\n\u001b[1;32m     37\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplits\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m.7\u001b[39m,\u001b[38;5;241m.15\u001b[39m)}\n\u001b[1;32m     39\u001b[0m sched_param \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfactor\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m.5\u001b[39m, \n\u001b[1;32m     40\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatience\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     41\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcooldown\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m}\n\u001b[0;32m---> 43\u001b[0m learn \u001b[38;5;241m=\u001b[39m \u001b[43mLearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTinyShakes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m              \u001b[49m\u001b[43mGPT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m              \u001b[49m\u001b[43mMetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m              \u001b[49m\u001b[43mSampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSelector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m              \u001b[49m\u001b[43mOptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m              \u001b[49m\u001b[43mScheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m              \u001b[49m\u001b[43mCriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m              \u001b[49m\u001b[43mmodel_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_param\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m              \u001b[49m\u001b[43mopt_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msched_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msched_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrit_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrit_param\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m              \u001b[49m\u001b[43mmetrics_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m              \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtinyshakes768\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m              \u001b[49m\u001b[43mload_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gpt/../cosmosis/learning.py:389\u001b[0m, in \u001b[0;36mLearn.__init__\u001b[0;34m(self, Datasets, Model, Sampler, Metrics, DataLoader, Optimizer, Scheduler, Criterion, ds_param, model_param, sample_param, opt_param, sched_param, crit_param, metrics_param, adapt, load_model, load_embed, save_model, batch_size, epochs, compile_model, gpu, weights_only, target)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m e\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler\u001b[38;5;241m.\u001b[39mshuffle_train_val_idx()\n\u001b[0;32m--> 389\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m no_grad():\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/gpt/../cosmosis/learning.py:480\u001b[0m, in \u001b[0;36mLearn.run\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    479\u001b[0m b_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(y_pred, y)\n\u001b[0;32m--> 480\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39me_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mb_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbs\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mmetric_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mappend(y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# put all together in a learner\n",
    "# (batch, d_seq, d_model)\n",
    "\n",
    "d_seq = 100 # dimension sequence\n",
    "d_vocab = 50304 # dimension vocabulary\n",
    "d_vec = 768 # dimension embedding vector\n",
    "d_model = 768 # dimension model input\n",
    "assert d_model == d_vec\n",
    "\n",
    "ds_param = {'train_param': {'transforms': {'tokens': [AsTensor()],\n",
    "                            'y': [AsTensor()],\n",
    "                            'position': [AsTensor()]},\n",
    "            'd_seq': d_seq,\n",
    "            #'n': 1000,\n",
    "                           }}\n",
    "\n",
    "model_param = {'d_model': d_model,\n",
    "               'd_vocab': d_vocab, \n",
    "               'n_head': 12, \n",
    "               'num_layers': 12,\n",
    "               'd_seq': d_seq,\n",
    "               'd_vec': d_vec,\n",
    "               'embed_param': {'tokens': (d_vocab, d_vec, None, True), \n",
    "                               'y': (d_vocab, d_vec, None, True),\n",
    "                               'position': (d_seq, d_vec, None, True)}} \n",
    "                                       \n",
    "metrics_param = {'metric_name': 'transformer',\n",
    "                 'report_interval': 1,\n",
    "                 'log_plot': False,\n",
    "                 'min_lr': .0025} # break if learning rate falls below                        \n",
    "             \n",
    "opt_param = {'lr': 0.01}\n",
    "\n",
    "crit_param = {}\n",
    "\n",
    "sample_param = {'set_seed': 88,\n",
    "                'splits': (.7,.15)}\n",
    "\n",
    "sched_param = {'factor': .5, \n",
    "               'patience': 2,\n",
    "               'cooldown': 2}\n",
    "\n",
    "learn = Learn([TinyShakes], \n",
    "              GPT,\n",
    "              Metrics=Metrics,\n",
    "              Sampler=Selector, \n",
    "              Optimizer=Adam, \n",
    "              Scheduler=ReduceLROnPlateau, \n",
    "              Criterion=CrossEntropyLoss,\n",
    "              model_param=model_param, ds_param=ds_param, sample_param=sample_param,\n",
    "              opt_param=opt_param, sched_param=sched_param, crit_param=crit_param,\n",
    "              metrics_param=metrics_param, \n",
    "              batch_size=32, epochs=10, gpu=True, save_model='tinyshakes768', \n",
    "              load_model=None, load_embed=False, target='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f741604a-078e-4156-8a8c-73ce10437b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(self.ds_idx):  1\n",
      "data.nbytes:  28\n",
      "CDataset created...\n",
      "applying _init_weights...\n",
      "GPT model loaded...\n",
      "number of model parameters:  38634240\n",
      "model loaded from state_dict...\n",
      "loading embedding weights...\n",
      "running model on gpu...\n",
      "\n",
      ".....................\n",
      "\n",
      "total learning time: 0:00:01.026713\n",
      "predictions:  [[' Mer beUSotheyr theAB the and Sign theolutePER FRIARARARARETETUSILLILLILLILL EL ELotheotheotheotheotheotheotheotheotheotheotheotheotheotheotheotheotheotheotheotheotheotheothe']]\n",
      "inference instance 2025-06-18 02:06:43.244648 complete and saved to csv...\n",
      "\n",
      ".....................\n",
      "\n",
      "total learning time: 0:00:01.115507\n",
      "predictions:  [['othe beUS supplyr the  the and LA the softlyPER ARARARARARARARARGGAR EL ELotheotheotheotheotheotheotheotheotheotheotheotheotheotheotheotheotheotheotheotheotheotheothe']]\n",
      "inference instance 2025-06-18 02:06:43.333442 complete and saved to csv...\n",
      "\n",
      ".....................\n",
      "\n",
      "total learning time: 0:00:01.195751\n",
      "predictions:  [['othe beUSothe V the IS the and all theolute LA  FRI FRI FRIINININININININININININININININININININININ VCHCHCHCHCHUUUUUCH']]\n",
      "inference instance 2025-06-18 02:06:43.413686 complete and saved to csv...\n",
      "\n",
      ".....................\n",
      "\n",
      "total learning time: 0:00:01.274259\n",
      "predictions:  [['othe beUSuralyr the. their and is thecil LA FRI FRI FRI FRI FRIETETAL Titus Titus Titus Titus Titus Titus Titus Titus Titus Titus Titus Titus Titus Titus Titus Titus Titus Titus Titus Titus Titus Titus Titus Titus Titus Titus Titus Titus Titus']]\n",
      "inference instance 2025-06-18 02:06:43.492194 complete and saved to csv...\n",
      "\n",
      ".....................\n",
      "\n",
      "total learning time: 0:00:01.359394\n",
      "predictions:  [['othe Simon Youothe my the POL the, BEN theolutePER FRIADICICININININININININININININININICICICICICICIC V VINCINCINCINCINCINCENTENTENTENT']]\n",
      "inference instance 2025-06-18 02:06:43.577329 complete and saved to csv...\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "d_gen = 50 # dimension generate number of tokens\n",
    "d_vocab = 50304 # dimension vocabulary\n",
    "d_vec = 768 # dimension embedding vector\n",
    "d_model = 768 # dimension model input\n",
    "d_pos = 50 # dimension positional encoding d_pos >= max(len(prompt_tokens), d_gen)\n",
    "\n",
    "assert d_model == d_vec\n",
    "\n",
    "\n",
    "ds_param = {'train_param': {'transforms': {'tokens': [AsTensor()],\n",
    "                                           'y': [AsTensor()],\n",
    "                                           'position': [AsTensor()]},\n",
    "                            'prompt': 'To be, or not to be, that is the question: '}\n",
    "           }\n",
    "\n",
    "model_param = {\n",
    "               'd_model': d_model,\n",
    "               'd_vocab': d_vocab, \n",
    "               'n_head': 12, \n",
    "               'num_layers': 12,\n",
    "               'd_gen': d_gen,\n",
    "               'd_vec': d_vec,\n",
    "               'temperature': 1000,\n",
    "               'top_k': 2,\n",
    "               'embed_param': {'tokens': (d_vocab, d_vec, None, True), \n",
    "                               #'y': (d_vocab, d_vec, None, True),\n",
    "                               'position': (d_pos, d_vec, None, True)},\n",
    "              } \n",
    "                                       \n",
    "metrics_param = {'metric_name': 'transformer'}                        \n",
    "             \n",
    "opt_param = {}\n",
    "\n",
    "crit_param = {}\n",
    "\n",
    "sample_param = {}\n",
    "\n",
    "sched_param = {}\n",
    "\n",
    "learn = Learn([TinyShakes], \n",
    "              GPT,\n",
    "              Metrics=Metrics,\n",
    "              Sampler=Selector, \n",
    "              Optimizer=None, \n",
    "              Scheduler=None, \n",
    "              Criterion=None, # no criterion implies inference\n",
    "              model_param=model_param, ds_param=ds_param, sample_param=sample_param,\n",
    "              opt_param=opt_param, sched_param=sched_param, crit_param=crit_param,\n",
    "              metrics_param=metrics_param, \n",
    "              batch_size=1, epochs=5, gpu=True, \n",
    "              load_model='tinyshakes768.pth', load_embed=True, target=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72674c76-3839-48eb-a1ef-689958185ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fe4265-f971-435c-8e4b-6d80cae3a069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
